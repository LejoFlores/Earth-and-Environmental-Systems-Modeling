{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set Up the Problem\n",
    "\n",
    "In this notebook, we will use the Snow-17 model as a vehicle to explore model sensitivity analysis and calibration. We will define ranges for the parameters that are used in our simplified version of Snow-17 – the degree day factor ($D_D$) and temperature threshold ($T_t$). We'll then create a random sample of parameter combinations that span this parameter space, run the model for each parameter combination, and then compute a variety of error metrics and visualize how they vary over the parameter space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numbers\n",
    "from scipy.stats import qmc # We'll use this to create parameter combinations\n",
    "\n",
    "# The name of file that contains forcing and observed SWE during every day of water year 2001-2020\n",
    "forcing_fname = 'EastRiver_hydro_data_2001-2020.csv'\n",
    "\n",
    "date_beg = '2000-10-01' # This is the first day of water year 2016\n",
    "date_end = '2020-09-30' # This is the last day of water year 2020\n",
    "\n",
    "dt = 1 # Time step [day]\n",
    "\n",
    "N_runs = 500 # Number of simulations to perform\n",
    "DD_lower = 1.0  # Lower bound for degree-day factor\n",
    "DD_upper = 10.5 # Upper bound for degree-day factor\n",
    "Tt_lower = -1.0 # Lower bound for temperature threshold\n",
    "Tt_upper = 7.0  # Upper bound for temperature threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Model - Snow 17\n",
    "\n",
    "New for this time, we will define our model as a function. A function is just a block of code that is a repeated set of calculations we might want to perform over and over again. This allows us to define the function once and call it many, many times. Here is a link where you can learn more about how to create functions. \n",
    "\n",
    "In our case, our function is called `Snow17`. It takes as input the time series of mean daily air temperature (`Ta` in °C) and daily total precipitation (`P` in mm) that are forcings of the model. It also takes as input the value of the degree-day melt factor ($D_D$) and threshold temperature ($T_t$) that will be used as parameter values for a single simulation. The `Snow17` function returns an array of simulated values of SWE, snow melt flux, and liquid precipitation for each time step of the input air temperature and precipitation. In the code that follows, we will call our function, `Snow17`, with many different combinations of parameters. \n",
    "\n",
    "Finally, pay attention to the three lines of code that call `assert`. Can you tell what this is doing? Why might it be helpful?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Snow17(Ta,P,DD,Tt):\n",
    "    \n",
    "    assert Ta.shape == P.shape, 'Precipitation and Temperature vectors must have the same shape'\n",
    "    assert isinstance(DD, numbers.Number), 'Degree day coefficient must be a scalar'\n",
    "    assert isinstance(Tt, numbers.Number), 'Temperature threshold must be a scalar'\n",
    "\n",
    "    Nt = np.max(Ta.shape)\n",
    "    \n",
    "    SWE_s17 = np.zeros(Ta.shape)\n",
    "    Sm_s17 = np.zeros(Ta.shape)\n",
    "    Pliq_s17 = np.zeros(Ta.shape)\n",
    "    \n",
    "    for i in np.arange(Nt):\n",
    "\n",
    "        P_i  = P[i] # The value of precipitation on this date\n",
    "        Ta_i = Ta[i] # The value of average air temperature on this date\n",
    "\n",
    "        # Initial conditions: we are starting when there should not be any appreciable snow in the watershed, \n",
    "        # so we will assume that SWE = 0. If you decide to run another date when there might be snow (e.g., Jan. 1)\n",
    "        # then you would need a more realistic value of SWE.\n",
    "        if(i==0):\n",
    "            SWE_i = 0.0 \n",
    "        else:\n",
    "            SWE_i = SWE_s17[i-1] # The initial SWE on these dates is simply the SWE from the day before. We will add snow or subtract melt.\n",
    "            \n",
    "        # If SWE is greater than zero, there *may* be snowmelt\n",
    "        if(SWE_i>0.0):\n",
    "            if(Ta_i>Tt): # If the air temperature is greater than the threshold, there **will** be melt\n",
    "                Sm_i = DD*(Ta_i-Tt) # Snowmelt via degree-day factor\n",
    "            else: # If the air temperature is below the threshold, there is no melt\n",
    "                Sm_i = 0.0 # No snowmelt if temperature does not exceed threshold\n",
    "        else: # If there is no SWE, by definition there is no snowmelt\n",
    "            Sm_i = 0.0\n",
    "        \n",
    "        # If there is precipitation, figure out its phase\n",
    "        if((P_i>0.0) and (Ta_i<=Tt)):\n",
    "            SWE_i += P_i # All precip will be added to SWE storage\n",
    "            Pliq_i = 0.0 # There is no liquid precipitation\n",
    "        elif((P_i>0.0) and (Ta_i>Tt)):\n",
    "            Pliq_i = P_i # All precipitation falls as liquid. NOTE: We are assuming rain does not melt snow!!!\n",
    "        else: # If there is no precipitation, there is nothing to accumulate\n",
    "            Pliq_i = 0.0\n",
    "        \n",
    "        SWE_s17[i] = np.max([SWE_i - Sm_i,0.0]) # Make sure we can only melt as much SWE as there is. This only matters at low SWE\n",
    "        Sm_s17[i] = Sm_i # Save the snowmelt... QUESTION: Is this something we can observe?!?!?!?!\n",
    "        Pliq_s17[i] = Pliq_i\n",
    "        \n",
    "        \n",
    "    return SWE_s17, Sm_s17, Pliq_s17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the Parameter Combinations \n",
    "\n",
    "We want to create a reasonably large number of possible parameter combinations that span the range of the feasible parameter space, without having to consider every parameter combination (we could do that, but it would be computationally expensive).\n",
    "\n",
    "*One* way to do this is to use something called Latin Hypercube (LHC) sampling. A latin hypercube gives us random samples between the lower and upper limit of our parameter space, but tries to make sure we have good \"coverage\" of the parameter sample space. That is, it tries to ensure that our parameters aren't too clustered in one part of the sample space. Below, we use an LHC sampler and plot all of the parameter combinations that we will be considering in our model runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = qmc.LatinHypercube(d=2)\n",
    "sample = sampler.random(N_runs)\n",
    "\n",
    "l_bounds = [DD_lower, Tt_lower]\n",
    "u_bounds = [DD_upper, Tt_upper]\n",
    "\n",
    "sample_scaled = qmc.scale(sample, l_bounds, u_bounds)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(sample_scaled[:,0],sample_scaled[:,1],'k.')\n",
    "plt.xlabel('Degree Day Factor [mm/day/°C]')\n",
    "plt.ylabel('Temperature Threshold [°C]')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the Forcing and Observed SWE Data\n",
    "\n",
    "Below, we use Pandas to load the forcing and observed SWE data. We also create a plot just to make sure that the precipitation and temperature data doesn't contain any obvious outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the forcing data data\n",
    "df_forcing = pd.read_csv(forcing_fname)\n",
    "\n",
    "# Reindex to create make sure that the index for the dataframe is a datetime64 object\n",
    "df_forcing['Date'] = pd.to_datetime(df_forcing['Date'],format='%Y-%m-%d')\n",
    "df_forcing.index = df_forcing['Date']\n",
    "\n",
    "# Here's what a pandas \"dataframe\" looks like:\n",
    "df_forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot temperature and precipitation during the period\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.bar(df_forcing['Date'][date_beg:date_end].values,df_forcing['pcp'][date_beg:date_end].values)\n",
    "plt.ylabel('Precipitation [mm/day]')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(df_forcing['Date'][date_beg:date_end].values,df_forcing['tair'][date_beg:date_end].values)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Air Temperature [°C]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Get Forcing Data and Create Storage Variables\n",
    "\n",
    "We will store the forcing data for the model simulation period in some `numpy` arrays, create a time series of dates during the simulation period, and set up storage containers for our simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForcingDates = df_forcing[date_beg:date_end]['Date'].values\n",
    "P_exp = df_forcing[date_beg:date_end]['pcp'].values\n",
    "Ta_exp = df_forcing[date_beg:date_end]['tair'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.date_range(start=date_beg, end=date_end, freq='1D')\n",
    "Nt = t.size\n",
    "\n",
    "SWE_exp = np.zeros((Nt,N_runs))\n",
    "Sm_exp = np.zeros((Nt,N_runs))\n",
    "Pliq_exp = np.zeros((Nt,N_runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the Model and Plot Results\n",
    "\n",
    "Below is our main modeling loop. See how much more succinct the code is now that we have put our Snow-17 code in its own function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in np.arange(N_runs):\n",
    "\n",
    "    # Create temporary storage containers for this replicate/run\n",
    "    SWE_rep = np.zeros((Nt))\n",
    "    Sm_rep = np.zeros((Nt))\n",
    "    Pliq_rep = np.zeros((Nt))\n",
    "    \n",
    "    # Get the degree-day factor and temperature threshold parameters for this run \n",
    "    DD_r = sample_scaled[r,0]\n",
    "    Tt_r = sample_scaled[r,1]\n",
    "    \n",
    "    # Call the Snow-17 model with input forcings and parameters\n",
    "    SWE_rep, Sm_rep, Pliq_rep = Snow17(Ta_exp,P_exp,DD_r,Tt_r)\n",
    "    \n",
    "    # Store the returned vectors in our containers for the whole experiment\n",
    "    SWE_exp[:,r] = SWE_rep\n",
    "    Sm_exp[:,r] = Sm_rep\n",
    "    Pliq_exp[:,r] = Pliq_rep # note: we're note going to do anything with this now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Plot All Model Simulations\n",
    "\n",
    "Below we will plot the entirety of our simulations of SWE and snowmelt flux. We will plot each run individually as well as the mean across all of the simulations – the so-called ensemble mean. Because we have observed SWE at the Snotel station, we will also plot that to compare to our SWE simulations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.figure(figsize=(10,14))\n",
    "\n",
    "for r in np.arange(N_runs):\n",
    "    plt.subplot(2,1,1)\n",
    "    if r == 0:\n",
    "        plt.plot(t,SWE_exp[:,r],'b-',label='Modeled SWE', linewidth=0.01)\n",
    "    else:\n",
    "        plt.plot(t,SWE_exp[:,r],'b-', linewidth=0.01)\n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    if r == 0:\n",
    "        plt.plot(t,Sm_exp[:,r],'b-',label='Modeled Snowmelt', linewidth=0.01)\n",
    "    else:\n",
    "        plt.plot(t,Sm_exp[:,r],'b-', linewidth=0.01)\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(t,np.mean(SWE_exp,1),'r-',label='Ensemble Mean', linewidth=2.0)\n",
    "plt.plot(t,df_forcing[date_beg:date_end]['SWE'].values,'k-',label='Observed SWE')\n",
    "plt.ylabel('Snow Water Equivalent [mm]')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(t,np.mean(Sm_exp,1),'r-',label='Ensemble Mean', linewidth=2.0)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Modeled Snowmelt [mm/day]')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute and Visualize Error Metrics\n",
    "\n",
    "Now we will compute a variety of individual and compound error metrics to assess the performance of our Snow-17 simulations. We will compute each metric for all combinations of parameters that we considered. We will then map how the value of each error metric varies with the value of the parameters. This will be done via contour maps of the value of the error metric in the parameter space. What we're looking to assess is whether there is a seemingly small area of parameter values that are associated with ideal values of our error metrics, or whether it's more complicated than that? The error metrics we are discussing are commonly used as __objective functions__ that we seek to minimize (like Root Mean Squared Error, RMSE) or maximize (like the Kling Gupta Efficiency, KGE). \n",
    "\n",
    "Specifically, as you review the plots below, think about the following questions for each error metric as an objective function:\n",
    "\n",
    "* How would you describe the shape of the objective function? Is it like a 2-D parabola? A saddle function? A hyperbola? Etc. \n",
    "* What does the shape of the objective function in our parameter space imply about whether an \"optimal\" or close to optimal set of parameters exists? Does only a small range of parameters minimize/maximize the objective function? Or is there a wide space? \n",
    "* If there is a really wide space of parameters that minimize/maximize the objective function, what does this imply about the model? Is this good or problematic? If it's problematic, how would we address the problematic nature.  \n",
    "\n",
    "### 6.1 Bias\n",
    "\n",
    "Recall that the bias is defined as:\n",
    "\n",
    "$$\n",
    "BIAS = \\mu_m - \\mu_o\n",
    "$$\n",
    "\n",
    "where $\\mu_m$ is the mean of the model predictions and $\\mu_o$ is the mean of the observations over the same period of time. Values of $BIAS > 0$ indicates parameter combinations that __overestimate__ SWE relative to observations, whereas values of $BIAS < 0$ indicate __underestimates__ of SWE. These are often (and sometimes unhelpfully) translated into the parlance of the model being simulated. So, $BIAS > 0$ might be called a __*wet bias*__ while $BIAS < 0$ might be called a __*dry bias*__. A final note, $BIAS$ has units that are the same as the variable we're assessing performance of (SWE in this case). That can give us an intuitive idea of how severe of an under- or over-estimate the model produces. For example a bias of -1 mm is intuitively small, but a bias of -250 mm is large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIAS = np.full((N_runs,1),np.nan)\n",
    "\n",
    "SWE_o = df_forcing[date_beg:date_end]['SWE'].values\n",
    "\n",
    "for r in np.arange(N_runs):\n",
    "    BIAS[r] = np.nanmean(SWE_exp[:,r]) - np.nanmean(SWE_o)\n",
    "    \n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(sample_scaled[:,0],sample_scaled[:,1],'k.')\n",
    "plt.tricontour(sample_scaled[:,0],sample_scaled[:,1],np.squeeze(BIAS),levels=15,colors='k')\n",
    "plt.tricontourf(sample_scaled[:,0],sample_scaled[:,1],np.squeeze(BIAS),levels=15,cmap='bwr_r')\n",
    "cb = plt.colorbar()\n",
    "cb.ax.set_title('$BIAS$ [mm]')\n",
    "plt.xlabel('Degree Day Factor [mm/day/°C]')\n",
    "plt.ylabel('Temperature Threshold [°C]')\n",
    "plt.grid('on')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Root Mean Squared Error (RMSE)\n",
    "\n",
    "Recall that the root mean squared error (RMSE) is defined as:\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(y_{m,i} - y_{o,i})^2}\n",
    "$$\n",
    "\n",
    "where $N$ is the number of simulation points (number of days in our simulation in this case), $y_{m,i}$ is the model-predicted SWE on day $i$ and $y_{o,i}$ is the observed SWE on day $i$. The ideal value of RMSE is 0, which would indicate a perfect simulation. Note that like the bias above, the RMSE has convenient units that are the same as the variable we're trying to predict – SWE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.full((N_runs,1),np.nan)\n",
    "\n",
    "SWE_o = df_forcing[date_beg:date_end]['SWE'].values\n",
    "\n",
    "for r in np.arange(N_runs):\n",
    "    RMSE[r] = np.sqrt(np.nanmean((SWE_exp[:,r] - SWE_o)**2))\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(sample_scaled[:,0],sample_scaled[:,1],'k.')\n",
    "plt.tricontour(sample_scaled[:,0],sample_scaled[:,1],np.squeeze(RMSE),levels=15,colors='k')\n",
    "plt.tricontourf(sample_scaled[:,0],sample_scaled[:,1],np.squeeze(RMSE),levels=15,cmap='rainbow')\n",
    "cb = plt.colorbar()\n",
    "cb.ax.set_title('RMSE [mm]')\n",
    "plt.xlabel('Degree Day Factor [mm/day/°C]')\n",
    "plt.ylabel('Temperature Threshold [°C]')\n",
    "plt.grid('on')\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Mean Absolute Error, MAE\n",
    "\n",
    "Recall that the definition of mean absolute error (MAE) is:\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{N}\\sum_{i=1}^{N}\\left| y_{i,m} - y_{o,i} \\right|\n",
    "$$\n",
    "\n",
    "Like RMSE, MAE is always positive, but does not punish extreme over- or under-predictions, unlike RMSE. Like bias and RMSE, also is associated with units that are the same as the variable we're assessing model performance of. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = np.full((N_runs,1),np.nan)\n",
    "\n",
    "SWE_o = df_forcing[date_beg:date_end]['SWE'].values\n",
    "\n",
    "for r in np.arange(N_runs):\n",
    "    MAE[r] = np.nanmean(np.abs(SWE_exp[:,r] - SWE_o))\n",
    "    \n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(sample_scaled[:,0],sample_scaled[:,1],'k.')\n",
    "plt.tricontour(sample_scaled[:,0],sample_scaled[:,1],np.squeeze(MAE),levels=15,colors='k')\n",
    "plt.tricontourf(sample_scaled[:,0],sample_scaled[:,1],np.squeeze(MAE),levels=15,cmap='rainbow')\n",
    "cb = plt.colorbar()\n",
    "cb.ax.set_title('MAE [mm]')\n",
    "plt.xlabel('Degree Day Factor [mm/day/°C]')\n",
    "plt.ylabel('Temperature Threshold [°C]')\n",
    "plt.grid('on')\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Pearson Correlation Coefficient\n",
    "\n",
    "The Pearson correlation coefficient is a measure of linear relationship between the model predictions of SWE on each day ($y_{m,i}$) and the corresponding observations of SWE on each day ($y_{o,i}$). It varies from -1 to 1. A value of 1 means that the model perfectly predicts the observations. The Pearson correlation coefficient can be calculated as:\n",
    "\n",
    "$$\n",
    "\\rho_{m,o} = \\frac{1}{N-1}\\frac{1}{\\sigma_m \\sigma_o}\\sum_{i=1}^{N}(y_{m,i} - \\mu_m)(y_{o,i} - \\mu_o)\n",
    "$$\n",
    "\n",
    "where $\\sigma_m$ is the standard deviation of the simulated values of SWE and $\\sigma_o$ is the standard deviation of observed values of SWE over the same period. Similarly, $\\mu_m$ is the average of the simulated values of SWE and $\\mu_o$ is the average of observed values of SWE over the same period.\n",
    "\n",
    "Note, in the below code, we are using the Pandas `df.corr()` function to compute the correlation coefficient. We do this because there are `nan` values in the observed SWE – before the Snotel station was installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rho = np.full((N_runs,1),np.nan)\n",
    "\n",
    "SWE_o = df_forcing[date_beg:date_end]['SWE'].values\n",
    "\n",
    "for r in np.arange(N_runs):\n",
    "    df_swe = pd.DataFrame({'SWE_m': SWE_exp[:,r], 'SWE_o': SWE_o})\n",
    "    Rho_mat = df_swe.corr().values\n",
    "    Rho[r] = Rho_mat[1,0]\n",
    "    \n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(sample_scaled[:,0],sample_scaled[:,1],'k.')\n",
    "plt.tricontour(sample_scaled[:,0],sample_scaled[:,1],np.squeeze(Rho),levels=15,colors='k')\n",
    "plt.tricontourf(sample_scaled[:,0],sample_scaled[:,1],np.squeeze(Rho),levels=15,cmap='rainbow')\n",
    "cb = plt.colorbar()\n",
    "cb.ax.set_title('Rho')\n",
    "plt.xlabel('Degree Day Factor [mm/day/°C]')\n",
    "plt.ylabel('Temperature Threshold [°C]')\n",
    "plt.grid('on')\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Kling Gupta Efficiency (KGE)\n",
    "\n",
    "The Kling Gupta Efficiency (KGE) score is a compound error metric that is often used as an objective function in the hydrologic sciences. It incorporates several measures of model performance, including a comparison of the modeled versus observed mean and standard deviation, and the Pearson correlation coefficient. \n",
    "\n",
    "$$\n",
    "KGE = 1 - \\sqrt{(\\rho - 1)^2 + (\\alpha - 1)^2 + (\\beta - 1)^2}\n",
    "$$\n",
    "\n",
    "where $\\rho$ is the correlation coefficient between the simulated and observed SWE, and\n",
    "\n",
    "$$\n",
    "\\beta = \\frac{\\mu_m}{\\mu_o},\n",
    "$$\n",
    "\n",
    "with $\\mu_m$ being the mean of the modeled values and $\\mu_o$ being the mean of the observations over the simulated time period, and\n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{\\sigma_m}{\\sigma_o},\n",
    "$$\n",
    "\n",
    "with $\\sigma_m$ being the mean of the modeled values and $\\sigma_o$ being the standard deviation of the observations over the simulated time period.\n",
    "\n",
    "Note that the ratio of the modeled to observed means is effectively a measure of bias, as an unbiased model would produce a value of $\\beta$ of 1. Similarly $\\alpha$ represents the degree to which the model produces too much $\\beta > 1$ or too little $\\beta < 1$ variability with respect to the observations. \n",
    "\n",
    "A perfect value of KGE would be a value of 1, given that this would require $\\rho = 1$, $\\alpha = 1$, and $\\beta = 1$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KGE = np.full((N_runs,1),np.nan)\n",
    "\n",
    "SWE_o = df_forcing[date_beg:date_end]['SWE'].values\n",
    "\n",
    "for r in np.arange(N_runs):\n",
    "    # Correlation coefficient\n",
    "    df_swe = pd.DataFrame({'SWE_m': SWE_exp[:,r], 'SWE_o': SWE_o})\n",
    "    Rho_mat = df_swe.corr().values\n",
    "    Rho = Rho_mat[1,0]\n",
    "    \n",
    "    # Normalized mean\n",
    "    beta = np.nanmean(SWE_exp[:,r]) / np.nanmean(SWE_o)\n",
    "    \n",
    "    # Normalized standard deviation\n",
    "    alpha = np.nanstd(SWE_exp[:,r]) / np.nanstd(SWE_o)\n",
    "    \n",
    "    # Compute Kling-Gupta Efficiency\n",
    "    KGE[r] = 1 - np.sqrt((Rho - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "    \n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(sample_scaled[:,0],sample_scaled[:,1],'k.')\n",
    "plt.tricontour(sample_scaled[:,0],sample_scaled[:,1],np.squeeze(KGE),levels=15,colors='k')\n",
    "plt.tricontourf(sample_scaled[:,0],sample_scaled[:,1],np.squeeze(KGE),levels=15,cmap='rainbow')\n",
    "cb = plt.colorbar()\n",
    "cb.ax.set_title('KGE')\n",
    "plt.xlabel('Degree Day Factor [mm/day/°C]')\n",
    "plt.ylabel('Temperature Threshold [°C]')\n",
    "plt.grid('on')\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Nash-Sutcliffe Efficiency (NSE)\n",
    "\n",
    "The Nash-Sutcliffe Efficiency (NSE) score is another compound measure that is used as a model performance measure and objective function in hydrology. It is defined as,\n",
    "\n",
    "$$\n",
    "NSE = 1 - \\frac{\\sum_{i=1}^{N}(y_{o,i} - y_{m,i})^2}{\\sum_{i=1}^{N}(y_{o,i} - \\mu_o)^2}\n",
    "$$\n",
    "\n",
    "where all terms are the same as defined above. Effectively, the NSE assesses the degree to which the model serves as a better predictor of the observations than simply taking the long-term mean of the observations. If we think of that long-term mean of the observations as a measure of \"climate\", NSE asks \"can the model beat an estimate based purely on climate?\" \n",
    "\n",
    "A perfect value of NSE is 1, which can only happen if the model predictions perfectly reproduce the observations. An NSE value of 0 indicates that the long-term observed mean is a better estimate than the mode and NSE can be negative, which would indicate that the model is __worse__ than the long-term mean as a prediction. \n",
    "\n",
    "A downside of NSE is that there's no objective sense of what constitutes a \"good\" model. In some quarters of the literature, authors report NSE > 0.85 is a skillful model, while others say that a skillful model can be associated with NSE > 0.60. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSE = np.full((N_runs,1),np.nan)\n",
    "\n",
    "SWE_o = df_forcing[date_beg:date_end]['SWE'].values\n",
    "\n",
    "for r in np.arange(N_runs):\n",
    "    NSE[r] = 1 - (np.nansum((SWE_o - SWE_exp[:,r])**2) / np.nansum((SWE_o - np.nanmean(SWE_o))**2))\n",
    "    \n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(sample_scaled[:,0],sample_scaled[:,1],'k.')\n",
    "plt.tricontour(sample_scaled[:,0],sample_scaled[:,1],np.squeeze(NSE),levels=15,colors='k')\n",
    "plt.tricontourf(sample_scaled[:,0],sample_scaled[:,1],np.squeeze(NSE),levels=15,cmap='rainbow')\n",
    "cb = plt.colorbar()\n",
    "cb.ax.set_title('NSE')\n",
    "plt.xlabel('Degree Day Factor [mm/day/°C]')\n",
    "plt.ylabel('Temperature Threshold [°C]')\n",
    "plt.grid('on')\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For subsequent visualization, we'll save the values of the parameters and associated KGE values to a pandas data frame and csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kge = pd.DataFrame(\n",
    "    {'DD_sa': sample_scaled[:,0], \n",
    "     'Tt_sa': sample_scaled[:,1],\n",
    "     'BIAS_sa': np.squeeze(BIAS),\n",
    "     'RMSE_sa': np.squeeze(RMSE),\n",
    "     'MAE_sa': np.squeeze(MAE),\n",
    "     'rho_sa': np.squeeze(Rho),\n",
    "     'KGE_sa': np.squeeze(KGE),\n",
    "     'NSE_sa': np.squeeze(NSE),\n",
    "    }\n",
    "    )\n",
    "df_kge.to_csv('Snow17_sensitivity_analysis.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geos505",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
